{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import random\n",
    "from game import Game\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TTTEnv(Env):\n",
    "    def __init__(self):\n",
    "        self.action_space = Discrete(9)\n",
    "        # Caused problems with keras-rl so I resorted to flattening it.\n",
    "        #self.observation_space = np.array([[Discrete(3)]*3,[Discrete(3)]*3,[Discrete(3)]*3])\n",
    "        self.observation_space = np.array([Discrete(3)]*9)\n",
    "        self.game = Game()\n",
    "        self.state = self.game.gameArray.flatten()\n",
    "        \n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "        done = False\n",
    "        self.game.printGame()\n",
    "        position = self.game.inputs[action]\n",
    "        if self.game.gameArray[position[0],position[1]] != 0:\n",
    "            reward -= 20\n",
    "            done = True\n",
    "        else:\n",
    "            self.game.gameArray[position[0],position[1]] = 1\n",
    "            gameOver, winner = self.game.checkWinGYM()\n",
    "            if winner == \"win\":\n",
    "                reward += 50\n",
    "                done = gameOver\n",
    "            elif winner == \"draw\":\n",
    "                reward += 10\n",
    "            elif winner == \"ingame\":\n",
    "                self.game.handleBotTurn()\n",
    "                gameOver, winner = self.game.checkWinGYM()\n",
    "                if winner == \"loss\":\n",
    "                    done = gameOver\n",
    "                    reward -= 50\n",
    "                elif winner == \"draw\":\n",
    "                    done = gameOver\n",
    "                    reward += 10\n",
    "        info = {}\n",
    "        return self.game.gameArray.flatten(), reward, done, info\n",
    "        \n",
    "    def render(self):\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = np.array([[0,0,0],[0,0,0],[0,0,0]])\n",
    "        self.game.resetGameArray()\n",
    "        return self.state\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 1 0\n",
      "0 0 2\n",
      "0 0 0\n",
      "Episode:1 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 1\n",
      "0 2 0\n",
      "0 2 0\n",
      "1 0 1\n",
      "0 2 0\n",
      "loss\n",
      "Episode:2 Score:-50\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "1 0 0\n",
      "0 0 2\n",
      "0 2 0\n",
      "1 0 1\n",
      "0 0 2\n",
      "1 2 0\n",
      "1 0 1\n",
      "2 0 2\n",
      "1 2 0\n",
      "1 2 1\n",
      "2 1 2\n",
      "Episode:3 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "1 0 0\n",
      "0 0 0\n",
      "0 0 2\n",
      "1 2 0\n",
      "0 0 0\n",
      "1 0 2\n",
      "Episode:4 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 1\n",
      "2 0 0\n",
      "0 0 1\n",
      "0 0 1\n",
      "2 2 0\n",
      "win\n",
      "Episode:5 Score:50\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 1 0\n",
      "2 0 0\n",
      "0 0 0\n",
      "Episode:6 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 2 0\n",
      "0 0 0\n",
      "0 1 0\n",
      "Episode:7 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 2\n",
      "0 1 0\n",
      "0 0 0\n",
      "0 0 2\n",
      "0 1 0\n",
      "1 2 0\n",
      "0 1 2\n",
      "0 1 0\n",
      "1 2 2\n",
      "Episode:8 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 1 0\n",
      "0 2 0\n",
      "0 0 1\n",
      "0 1 0\n",
      "0 2 2\n",
      "loss\n",
      "Episode:9 Score:-50\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 2\n",
      "0 0 1\n",
      "0 0 0\n",
      "0 0 2\n",
      "1 2 1\n",
      "Episode:10 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 2 0\n",
      "1 0 0\n",
      "0 0 0\n",
      "0 2 1\n",
      "1 0 0\n",
      "0 0 2\n",
      "0 2 1\n",
      "1 1 2\n",
      "0 0 2\n",
      "win\n",
      "Episode:11 Score:50\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "1 0 0\n",
      "0 0 2\n",
      "0 0 0\n",
      "Episode:12 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 2 1\n",
      "0 0 0\n",
      "0 0 0\n",
      "2 2 1\n",
      "0 0 1\n",
      "Episode:13 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "1 2 0\n",
      "Episode:14 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "1 0 0\n",
      "0 2 0\n",
      "0 0 0\n",
      "1 1 0\n",
      "0 2 0\n",
      "2 0 0\n",
      "Episode:15 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "2 0 1\n",
      "0 0 0\n",
      "0 0 0\n",
      "Episode:16 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 2 0\n",
      "0 0 0\n",
      "0 0 1\n",
      "1 2 0\n",
      "2 0 0\n",
      "0 0 1\n",
      "1 2 0\n",
      "2 0 2\n",
      "1 0 1\n",
      "Episode:17 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "1 2 0\n",
      "Episode:18 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 2 0\n",
      "0 1 0\n",
      "0 0 0\n",
      "1 2 2\n",
      "0 1 0\n",
      "0 0 0\n",
      "win\n",
      "Episode:19 Score:50\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 1 0\n",
      "2 0 0\n",
      "0 0 0\n",
      "Episode:20 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 2 0\n",
      "0 0 1\n",
      "Episode:21 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 2\n",
      "1 0 0\n",
      "Episode:22 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 2 0\n",
      "1 0 0\n",
      "1 2 0\n",
      "0 2 0\n",
      "1 0 0\n",
      "win\n",
      "Episode:23 Score:50\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 1 0\n",
      "0 2 0\n",
      "0 0 0\n",
      "0 1 2\n",
      "0 2 0\n",
      "0 1 0\n",
      "loss\n",
      "Episode:24 Score:-50\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 1 0\n",
      "0 0 0\n",
      "2 0 0\n",
      "0 1 0\n",
      "0 1 0\n",
      "2 2 0\n",
      "0 1 2\n",
      "1 1 0\n",
      "2 2 0\n",
      "Episode:25 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 2 1\n",
      "0 0 0\n",
      "0 0 0\n",
      "1 2 1\n",
      "0 0 2\n",
      "Episode:26 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 2\n",
      "0 0 0\n",
      "0 1 0\n",
      "0 0 2\n",
      "1 0 2\n",
      "0 1 0\n",
      "2 0 2\n",
      "1 0 2\n",
      "0 1 1\n",
      "2 0 2\n",
      "1 1 2\n",
      "2 1 1\n",
      "Episode:27 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 1 0\n",
      "0 0 0\n",
      "0 2 0\n",
      "Episode:28 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 2 0\n",
      "0 0 0\n",
      "0 1 0\n",
      "Episode:29 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 1 0\n",
      "0 0 0\n",
      "0 2 0\n",
      "2 1 0\n",
      "1 0 0\n",
      "0 2 0\n",
      "2 1 1\n",
      "1 0 2\n",
      "0 2 0\n",
      "Episode:30 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 2 0\n",
      "0 0 1\n",
      "2 0 1\n",
      "0 2 0\n",
      "0 0 1\n",
      "2 0 1\n",
      "0 2 0\n",
      "2 1 1\n",
      "win\n",
      "Episode:31 Score:50\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 1\n",
      "2 0 0\n",
      "1 2 0\n",
      "0 0 1\n",
      "2 0 0\n",
      "Episode:32 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 1\n",
      "0 2 0\n",
      "0 0 0\n",
      "0 2 1\n",
      "1 2 0\n",
      "1 0 2\n",
      "0 2 1\n",
      "1 2 0\n",
      "Episode:33 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 2\n",
      "0 0 1\n",
      "0 0 0\n",
      "0 0 2\n",
      "0 0 1\n",
      "2 1 0\n",
      "Episode:34 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "2 0 0\n",
      "0 0 1\n",
      "0 0 0\n",
      "Episode:35 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "1 0 0\n",
      "0 2 0\n",
      "0 0 0\n",
      "1 1 0\n",
      "0 2 2\n",
      "2 1 0\n",
      "1 1 0\n",
      "0 2 2\n",
      "2 1 0\n",
      "1 1 2\n",
      "1 2 2\n",
      "Episode:36 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 2\n",
      "0 0 0\n",
      "0 0 1\n",
      "1 0 2\n",
      "0 0 2\n",
      "0 0 1\n",
      "1 2 2\n",
      "0 0 2\n",
      "1 0 1\n",
      "Episode:37 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 2 0\n",
      "0 1 0\n",
      "0 0 0\n",
      "0 2 0\n",
      "0 1 0\n",
      "0 1 2\n",
      "Episode:38 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "1 0 0\n",
      "0 0 0\n",
      "2 0 0\n",
      "1 0 0\n",
      "2 0 1\n",
      "2 0 0\n",
      "1 1 0\n",
      "2 2 1\n",
      "2 0 0\n",
      "1 1 0\n",
      "2 2 1\n",
      "2 2 1\n",
      "Episode:39 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "1 0 0\n",
      "0 0 2\n",
      "0 0 0\n",
      "Episode:40 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "2 0 0\n",
      "0 0 0\n",
      "0 1 0\n",
      "2 0 1\n",
      "0 0 2\n",
      "0 1 0\n",
      "2 0 1\n",
      "0 1 2\n",
      "2 1 0\n",
      "Episode:41 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "2 1 0\n",
      "1 0 0\n",
      "0 0 2\n",
      "2 1 0\n",
      "1 0 0\n",
      "1 0 2\n",
      "2 1 2\n",
      "Episode:42 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "2 0 0\n",
      "0 1 0\n",
      "0 0 0\n",
      "2 1 0\n",
      "0 1 0\n",
      "0 0 2\n",
      "2 1 0\n",
      "0 1 2\n",
      "1 0 2\n",
      "win\n",
      "Episode:43 Score:50\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 1 2\n",
      "0 2 1\n",
      "0 0 0\n",
      "0 1 2\n",
      "0 2 1\n",
      "0 1 0\n",
      "2 1 2\n",
      "2 2 1\n",
      "0 1 1\n",
      "2 1 2\n",
      "win\n",
      "Episode:44 Score:50\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 2 0\n",
      "1 0 0\n",
      "0 0 0\n",
      "0 2 0\n",
      "1 0 0\n",
      "2 1 0\n",
      "1 2 0\n",
      "1 2 0\n",
      "2 1 0\n",
      "Episode:45 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 1\n",
      "0 2 0\n",
      "0 2 0\n",
      "0 1 1\n",
      "0 2 0\n",
      "Episode:46 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "2 1 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "Episode:47 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 1\n",
      "2 0 0\n",
      "0 0 0\n",
      "0 2 1\n",
      "2 1 0\n",
      "loss\n",
      "Episode:48 Score:-50\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 1 2\n",
      "0 0 0\n",
      "0 0 1\n",
      "2 1 2\n",
      "0 0 0\n",
      "Episode:49 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 1 0\n",
      "2 0 0\n",
      "0 0 0\n",
      "1 1 0\n",
      "2 0 0\n",
      "0 2 0\n",
      "1 1 0\n",
      "2 2 0\n",
      "0 2 1\n",
      "Episode:50 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 2\n",
      "1 0 0\n",
      "0 0 0\n",
      "Episode:51 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "1 0 0\n",
      "0 0 0\n",
      "0 2 0\n",
      "1 0 0\n",
      "0 2 0\n",
      "0 2 1\n",
      "Episode:52 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "1 2 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "1 2 0\n",
      "1 0 0\n",
      "0 2 0\n",
      "win\n",
      "Episode:53 Score:50\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 1\n",
      "0 0 0\n",
      "2 0 0\n",
      "2 0 1\n",
      "1 0 0\n",
      "2 0 0\n",
      "2 0 1\n",
      "1 1 2\n",
      "2 0 0\n",
      "Episode:54 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 2\n",
      "1 0 0\n",
      "0 0 0\n",
      "0 0 2\n",
      "1 1 2\n",
      "0 0 0\n",
      "0 0 2\n",
      "1 1 2\n",
      "1 2 0\n",
      "2 1 2\n",
      "1 1 2\n",
      "1 2 0\n",
      "Episode:55 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "1 0 0\n",
      "0 2 0\n",
      "1 0 0\n",
      "1 0 0\n",
      "0 2 2\n",
      "Episode:56 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 2 0\n",
      "1 0 0\n",
      "0 0 0\n",
      "0 2 2\n",
      "1 1 0\n",
      "0 0 0\n",
      "loss\n",
      "Episode:57 Score:-50\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 1\n",
      "0 2 0\n",
      "0 0 0\n",
      "0 0 1\n",
      "1 2 2\n",
      "0 1 0\n",
      "0 2 1\n",
      "1 2 2\n",
      "loss\n",
      "Episode:58 Score:-50\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "1 0 2\n",
      "0 0 0\n",
      "0 1 0\n",
      "1 2 2\n",
      "0 0 0\n",
      "Episode:59 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "1 0 0\n",
      "0 2 0\n",
      "Episode:60 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "1 0 0\n",
      "0 0 0\n",
      "0 0 2\n",
      "1 1 0\n",
      "0 0 0\n",
      "0 2 2\n",
      "Episode:61 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 2\n",
      "0 1 0\n",
      "0 0 0\n",
      "0 0 2\n",
      "0 1 2\n",
      "1 0 0\n",
      "loss\n",
      "Episode:62 Score:-50\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "2 0 0\n",
      "0 0 0\n",
      "0 0 1\n",
      "2 0 0\n",
      "0 1 0\n",
      "2 0 1\n",
      "2 0 1\n",
      "0 1 0\n",
      "2 2 1\n",
      "loss\n",
      "Episode:63 Score:-50\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 1\n",
      "0 2 0\n",
      "0 0 1\n",
      "0 2 1\n",
      "0 2 0\n",
      "loss\n",
      "Episode:64 Score:-50\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 2 0\n",
      "0 0 1\n",
      "Episode:65 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 2\n",
      "0 0 0\n",
      "0 0 1\n",
      "0 0 2\n",
      "0 0 2\n",
      "0 1 1\n",
      "1 0 2\n",
      "2 0 2\n",
      "0 1 1\n",
      "win\n",
      "Episode:66 Score:50\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 2 0\n",
      "0 0 1\n",
      "0 1 2\n",
      "0 2 0\n",
      "0 0 1\n",
      "Episode:67 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 1\n",
      "2 0 0\n",
      "0 0 0\n",
      "Episode:68 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 1\n",
      "0 0 0\n",
      "2 0 0\n",
      "1 0 1\n",
      "0 0 0\n",
      "2 0 2\n",
      "win\n",
      "Episode:69 Score:50\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "1 0 0\n",
      "2 0 0\n",
      "0 0 0\n",
      "1 0 1\n",
      "2 0 0\n",
      "0 0 2\n",
      "Episode:70 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "1 0 0\n",
      "0 0 0\n",
      "2 0 0\n",
      "1 2 0\n",
      "0 1 0\n",
      "2 0 0\n",
      "1 2 2\n",
      "0 1 1\n",
      "2 0 0\n",
      "win\n",
      "Episode:71 Score:50\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 1 2\n",
      "0 0 1\n",
      "0 0 0\n",
      "2 1 2\n",
      "Episode:72 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 2\n",
      "0 0 0\n",
      "0 0 1\n",
      "0 2 2\n",
      "0 1 0\n",
      "0 0 1\n",
      "Episode:73 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "2 0 1\n",
      "0 0 0\n",
      "0 2 0\n",
      "2 0 1\n",
      "0 0 1\n",
      "2 2 0\n",
      "2 0 1\n",
      "0 1 1\n",
      "Episode:74 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "2 0 0\n",
      "0 0 0\n",
      "1 0 0\n",
      "2 0 0\n",
      "0 0 1\n",
      "1 0 2\n",
      "Episode:75 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "2 0 0\n",
      "0 1 0\n",
      "0 0 0\n",
      "2 2 0\n",
      "0 1 0\n",
      "0 0 1\n",
      "2 2 0\n",
      "0 1 0\n",
      "1 2 1\n",
      "Episode:76 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "1 0 0\n",
      "0 2 0\n",
      "0 1 0\n",
      "1 2 0\n",
      "0 2 0\n",
      "Episode:77 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 1\n",
      "0 0 0\n",
      "2 0 0\n",
      "Episode:78 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 2 0\n",
      "0 0 0\n",
      "0 0 1\n",
      "Episode:79 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 2\n",
      "0 0 0\n",
      "0 0 1\n",
      "1 0 2\n",
      "0 0 0\n",
      "0 2 1\n",
      "Episode:80 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 1 2\n",
      "0 0 0\n",
      "0 1 0\n",
      "2 1 2\n",
      "0 0 0\n",
      "win\n",
      "Episode:81 Score:50\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "1 0 0\n",
      "2 0 0\n",
      "Episode:82 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "1 2 0\n",
      "0 0 0\n",
      "0 2 0\n",
      "1 2 1\n",
      "0 0 0\n",
      "Episode:83 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 1\n",
      "0 0 2\n",
      "Episode:84 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 1 0\n",
      "2 0 0\n",
      "0 0 0\n",
      "2 1 0\n",
      "2 0 0\n",
      "0 1 0\n",
      "Episode:85 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 1 2\n",
      "0 0 2\n",
      "0 0 1\n",
      "0 1 2\n",
      "Episode:86 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "2 0 0\n",
      "0 0 0\n",
      "0 1 0\n",
      "2 0 0\n",
      "2 0 0\n",
      "0 1 1\n",
      "Episode:87 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "2 0 0\n",
      "0 1 0\n",
      "0 0 0\n",
      "2 0 0\n",
      "1 1 2\n",
      "Episode:88 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "2 0 1\n",
      "0 0 0\n",
      "0 0 0\n",
      "Episode:89 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "1 0 2\n",
      "0 0 0\n",
      "0 0 0\n",
      "1 0 2\n",
      "0 1 2\n",
      "Episode:90 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "2 0 0\n",
      "0 0 1\n",
      "0 0 0\n",
      "2 1 0\n",
      "0 2 1\n",
      "win\n",
      "Episode:91 Score:50\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "2 0 0\n",
      "0 1 0\n",
      "0 0 0\n",
      "2 0 2\n",
      "0 1 1\n",
      "Episode:92 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 1\n",
      "0 0 2\n",
      "0 0 0\n",
      "0 0 1\n",
      "2 0 2\n",
      "0 1 0\n",
      "1 2 1\n",
      "2 0 2\n",
      "0 1 0\n",
      "1 2 1\n",
      "2 1 2\n",
      "2 1 0\n",
      "Episode:93 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 1\n",
      "0 0 2\n",
      "0 0 0\n",
      "Episode:94 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 2 0\n",
      "0 0 0\n",
      "0 1 0\n",
      "Episode:95 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 2 1\n",
      "0 1 2\n",
      "0 0 0\n",
      "0 2 1\n",
      "0 1 2\n",
      "0 0 1\n",
      "2 2 1\n",
      "Episode:96 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 1 0\n",
      "0 0 2\n",
      "1 0 0\n",
      "2 1 0\n",
      "0 0 2\n",
      "1 0 0\n",
      "2 1 0\n",
      "1 2 2\n",
      "win\n",
      "Episode:97 Score:50\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "2 1 0\n",
      "0 0 0\n",
      "Episode:98 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 2 0\n",
      "0 0 1\n",
      "0 0 0\n",
      "0 2 0\n",
      "1 2 1\n",
      "0 0 0\n",
      "2 2 1\n",
      "1 2 1\n",
      "0 0 0\n",
      "Episode:99 Score:-20\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "2 0 0\n",
      "0 0 0\n",
      "0 0 1\n",
      "2 2 0\n",
      "1 0 0\n",
      "0 0 1\n",
      "loss\n",
      "Episode:100 Score:-50\n"
     ]
    }
   ],
   "source": [
    "env = TTTEnv()\n",
    "episodes = 100\n",
    "for episode in range(1, episodes+1):\n",
    "    #env.render()\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "    print(\"Episode:{} Score:{}\".format(episode,score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Deep Learning Model (GYM)\n",
    "(INCOMPLETE, CANT FIND WORKABLE SOLUTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join('Training', 'Logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "[Discrete(3) Discrete(3) Discrete(3) Discrete(3) Discrete(3) Discrete(3)\n Discrete(3) Discrete(3) Discrete(3)] observation space is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-95623ff665ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0menv2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDummyVecEnv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDQN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'MlpPolicy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensorboard_log\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlog_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\dqn\\dqn.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, policy, env, learning_rate, buffer_size, learning_starts, batch_size, tau, gamma, train_freq, gradient_steps, replay_buffer_class, replay_buffer_kwargs, optimize_memory_usage, target_update_interval, exploration_fraction, exploration_initial_eps, exploration_final_eps, max_grad_norm, tensorboard_log, create_eval_env, policy_kwargs, verbose, seed, device, _init_setup_model)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_init_setup_model\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setup_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_setup_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\dqn\\dqn.py\u001b[0m in \u001b[0;36m_setup_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_setup_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDQN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setup_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_aliases\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m         self.exploration_schedule = get_linear_fn(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py\u001b[0m in \u001b[0;36m_setup_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m             self.replay_buffer = self.replay_buffer_class(\n\u001b[0m\u001b[0;32m    213\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\buffers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, buffer_size, observation_space, action_space, device, n_envs, optimize_memory_usage, handle_timeout_termination)\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[0mhandle_timeout_termination\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m     ):\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mReplayBuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobservation_space\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_space\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_envs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_envs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mn_envs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Replay buffer only support single environment for now\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\buffers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, buffer_size, observation_space, action_space, device, n_envs)\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation_space\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobservation_space\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction_space\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobs_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_obs_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_action_dim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\preprocessing.py\u001b[0m in \u001b[0;36mget_obs_shape\u001b[1;34m(observation_space)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{observation_space} observation space is not supported\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: [Discrete(3) Discrete(3) Discrete(3) Discrete(3) Discrete(3) Discrete(3)\n Discrete(3) Discrete(3) Discrete(3)] observation space is not supported"
     ]
    }
   ],
   "source": [
    "env2 = DummyVecEnv([lambda: env])\n",
    "model = DQN('MlpPolicy', env2, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Deep Learning Model (Keras-RL)\n",
    "(INCOMPLETE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9,)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    model = Sequential()\n",
    "    #model.add(Flatten(states))\n",
    "    model.add(Dense(24, activation=\"relu\",input_shape=states))\n",
    "    model.add(Dense(24, activation=\"relu\"))\n",
    "    model.add(Dense(actions, activation=\"linear\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 24)                240       \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 9)                 225       \n",
      "=================================================================\n",
      "Total params: 1,065\n",
      "Trainable params: 1,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(states, actions)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 9])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "                    nb_actions=actions, nb_steps_warmup=10, \n",
    "                    target_model_update=1e-2)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n",
      "Interval 1 (0 steps performed)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_16_input to have 2 dimensions, but got array with shape (1, 1, 3, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-b58ebd9470f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdqn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_agent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"mae\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\rl\\core.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, env, nb_steps, action_repetition, callbacks, verbose, visualize, nb_max_start_steps, start_step_policy, log_interval, nb_max_episode_steps)\u001b[0m\n\u001b[0;32m    166\u001b[0m                 \u001b[1;31m# This is were all of the work happens. We first perceive and compute the action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m                 \u001b[1;31m# (forward step) and then use the reward to improve (backward step).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m                     \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\rl\\agents\\dqn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[1;31m# Select an action.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_recent_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m         \u001b[0mq_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_q_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mq_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\rl\\agents\\dqn.py\u001b[0m in \u001b[0;36mcompute_q_values\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcompute_q_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m         \u001b[0mq_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_batch_q_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mq_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnb_actions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mq_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\rl\\agents\\dqn.py\u001b[0m in \u001b[0;36mcompute_batch_q_values\u001b[1;34m(self, state_batch)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcompute_batch_q_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_state_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mq_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mq_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnb_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mq_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36mpredict_on_batch\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m   1185\u001b[0m           ' tf.distribute.Strategy.')\n\u001b[0;32m   1186\u001b[0m     \u001b[1;31m# Validate and standardize user data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1187\u001b[1;33m     inputs, _, _ = self._standardize_user_data(\n\u001b[0m\u001b[0;32m   1188\u001b[0m         x, extract_tensors_from_dataset=True)\n\u001b[0;32m   1189\u001b[0m     \u001b[1;31m# If `self._distribution_strategy` is True, then we are in a replica context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2327\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2329\u001b[1;33m     return self._standardize_tensors(\n\u001b[0m\u001b[0;32m   2330\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2331\u001b[0m         \u001b[0mrun_eagerly\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrun_eagerly\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[1;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[0;32m   2355\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2356\u001b[0m       \u001b[1;31m# TODO(fchollet): run static checks with dataset output shape(s).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2357\u001b[1;33m       x = training_utils_v1.standardize_input_data(\n\u001b[0m\u001b[0;32m   2358\u001b[0m           \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2359\u001b[0m           \u001b[0mfeed_input_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training_utils_v1.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    631\u001b[0m         \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshapes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_shape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 633\u001b[1;33m           raise ValueError('Error when checking ' + exception_prefix +\n\u001b[0m\u001b[0;32m    634\u001b[0m                            \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    635\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected dense_16_input to have 2 dimensions, but got array with shape (1, 1, 3, 3)"
     ]
    }
   ],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=[\"mae\"])\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
